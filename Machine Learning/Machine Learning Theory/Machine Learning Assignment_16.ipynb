{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82c50a4",
   "metadata": {},
   "source": [
    "1. In a linear equation, what is the difference between a dependent variable and an independent variable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f334e",
   "metadata": {},
   "source": [
    "In a linear equation, the dependent variable is the output or response variable that is being predicted or explained by one or more independent variables. The independent variable is the input variable that is used to explain or predict the value of the dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3be1d",
   "metadata": {},
   "source": [
    "2. What is the concept of simple linear regression? Give a specific example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d39fdd",
   "metadata": {},
   "source": [
    "Simple linear regression is a statistical method used to establish a relationship between two continuous variables, where one variable (the dependent variable) is assumed to be linearly related to the other variable (the independent variable) through a straight line.\n",
    "\n",
    "For example, suppose a company wants to predict the sales of their product based on the advertising expenditure. In this case, sales would be the dependent variable, while advertising expenditure would be the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90836e79",
   "metadata": {},
   "source": [
    "3. In a linear regression, define the slope.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc793e",
   "metadata": {},
   "source": [
    "In linear regression, the slope is a measure of the steepness of the relationship between the independent variable and the dependent variable.\n",
    "\n",
    "slope = (covariance of x and y) / (variance of x)\n",
    "\n",
    "where x is the independent variable and y is the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff447a",
   "metadata": {},
   "source": [
    "4. Determine the graph's slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65476676",
   "metadata": {},
   "source": [
    "There seems to be an error in the question. Both points have the same y-coordinate (2), which means they lie on a horizontal line, and the slope of a horizontal line is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e898b",
   "metadata": {},
   "source": [
    "5. In linear regression, what are the conditions for a positive slope?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72464bd1",
   "metadata": {},
   "source": [
    "The conditions for a positive slope are:\n",
    "\n",
    "The independent variable and the dependent variable have a positive correlation, which means that as the independent variable increases, the dependent variable tends to increase as well.\n",
    "\n",
    "The residuals (the difference between the actual values and the predicted values) have a normal distribution and are randomly scattered around the regression line.\n",
    "\n",
    "There are no influential outliers that can skew the slope of the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfbc779",
   "metadata": {},
   "source": [
    "6. In linear regression, what are the conditions for a negative slope?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73b8f4",
   "metadata": {},
   "source": [
    "In linear regression, the conditions for a negative slope are:\n",
    "\n",
    "The independent variable and dependent variable have a negative correlation, which means that as the independent variable increases, the dependent variable decreases.\n",
    "\n",
    "The sum of the differences between the actual y-values and the predicted y-values is minimized.\n",
    "\n",
    "The residuals, which are the differences between the actual y-values and the predicted y-values, are normally distributed around the regression line with a mean of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ea4c6",
   "metadata": {},
   "source": [
    "\n",
    "7. What is multiple linear regression and how does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67537af6",
   "metadata": {},
   "source": [
    "Multiple linear regression is a technique that helps to predict the value of a dependent variable based on the values of two or more independent variables.\n",
    "\n",
    "The multiple linear regression model estimates the values of the regression coefficients using a technique called ordinary least squares (OLS) regression. The OLS method minimizes the sum of the squared differences between the observed and predicted values of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00037a4c",
   "metadata": {},
   "source": [
    "8. In multiple linear regression, define the number of squares due to error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509725f9",
   "metadata": {},
   "source": [
    "n multiple linear regression, the sum of squares due to error (SSE) is the sum of the squared differences between the actual and predicted values of the dependent variable. The formula for SSE is:\n",
    "\n",
    "SSE = ∑(Yᵢ - Ȳ)²\n",
    "\n",
    "Where:\n",
    "\n",
    "Yᵢ is the actual value of the dependent variable for observation i\n",
    "\n",
    "Ȳ is the mean of the dependent variable\n",
    "\n",
    "The goal of multiple linear regression is to minimize SSE by finding the best fitting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c800ea9",
   "metadata": {},
   "source": [
    "9. In multiple linear regression, define the number of squares due to regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804dc09e",
   "metadata": {},
   "source": [
    "In multiple linear regression, the sum of squares due to regression (SSR) is the sum of the squared differences between the predicted values of y (i.e., the values obtained from the regression equation) and the mean of y.\n",
    "\n",
    "The formula for SSR is:\n",
    "\n",
    "SSR = Σ(ŷi - ȳ)²\n",
    "\n",
    "where:\n",
    "\n",
    "Σ is the summation symbol\n",
    "\n",
    "ŷi is the predicted value of y for the ith observation\n",
    "\n",
    "ȳ is the mean of y\n",
    "\n",
    "SSR can also be calculated as the difference between the total sum of squares (SST) and the sum of squares due to error (SSE):\n",
    "\n",
    "SSR = SST - SSE\n",
    "\n",
    "where:\n",
    "\n",
    "SST is the total sum of squares, which measures the total variability in y\n",
    "SSE is the sum of squares due to error, which measures the variability in y that is not explained by the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89eebe",
   "metadata": {},
   "source": [
    "\n",
    "10. In a regression equation, what is multicollinearity?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483eda65",
   "metadata": {},
   "source": [
    "Multicollinearity is a phenomenon in a regression equation where independent variables are highly correlated with each other.This will be difficult to interpret the relationship between each independent variable and the dependent variable accurately. It can also lead to overfitting, where the model fits the training data too closely and fails to generalize well to new data. In general, multicollinearity is a problem to be avoided in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74e951",
   "metadata": {},
   "source": [
    "11. What is heteroskedasticity, and what does it mean?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed970d",
   "metadata": {},
   "source": [
    "Heteroscedasticity is a term used in regression analysis to describe the situation where the variability of the errors is different for different levels of the independent variable.Heteroscedasticity can lead to biased estimates of the standard errors of the regression coefficients, which can affect the significance tests and confidence intervals of the regression analysis. It can also affect the predictive power of the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05396f70",
   "metadata": {},
   "source": [
    "12. Describe the concept of ridge regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd6d6c",
   "metadata": {},
   "source": [
    "Ridge regression is a regularization technique used in linear regression to prevent overfitting of the model. It involves adding a penalty term to the least squares objective function, which shrinks the regression coefficients towards zero. By adding this penalty term, ridge regression reduces the variance of the estimates of the regression coefficients, at the expense of a slight increase in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee7aa1",
   "metadata": {},
   "source": [
    "13. Describe the concept of lasso regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5105f",
   "metadata": {},
   "source": [
    "Lasso regression is a type of linear regression that performs both variable selection and regularization in order to prevent overfitting. The acronym \"lasso\" stands for \"Least Absolute Shrinkage and Selection Operator.\" Lasso regression is especially useful when dealing with high-dimensional data, where the number of features is much larger than the number of samples. By eliminating irrelevant features, lasso regression can help improve the model's performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523397df",
   "metadata": {},
   "source": [
    "14. What is polynomial regression and how does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd3b75",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis that models the relationship between the independent variable x and the dependent variable y as an nth degree polynomial function. \n",
    "\n",
    "The polynomial function can be expressed as:\n",
    "\n",
    "y = b0 + b1x + b2x^2 + ... + bnx^n\n",
    "\n",
    "where y is the dependent variable, x is the independent variable, b0, b1, b2,..., bn are the coefficients of the polynomial, and n is the degree of the polynomial.\n",
    "\n",
    "Polynomial regression is useful when the relationship between the independent and dependent variables is not linear. It allows for more flexibility in modeling the relationship between the variables.\n",
    "\n",
    "Using higher degree polynomials can lead to overfitting, where the model fits the training data too closely and performs poorly on new, unseen data. Therefore, selecting the appropriate degree of polynomial is crucial in achieving a good balance between model complexity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0eb0af",
   "metadata": {},
   "source": [
    "15. Describe the basis function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad7fab",
   "metadata": {},
   "source": [
    "In machine learning, a basis function is a mathematical function used to transform input data into a higher-dimensional space. In the context of regression analysis, a basis function transforms the input features into a set of new features that are then used to make predictions. For example, a linear regression model might use a polynomial basis function to transform the input feature x into x, x^2, x^3, etc. This enables the model to capture nonlinear relationships between the input and output variables. Basis functions are commonly used in kernel methods such as support vector machines (SVMs) and Gaussian processes (GPs). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fc169",
   "metadata": {},
   "source": [
    "16. Describe how logistic regression works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944cc2eb",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical method used for binary classification problems, where the response variable can take only two possible values. The logistic regression model uses a logistic function (also called the sigmoid function) to model the probability of the positive class. logistic regression works by fitting a logistic function to the input features, estimating the coefficients using maximum likelihood estimation, and using the function to predict the probability of the positive outcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
