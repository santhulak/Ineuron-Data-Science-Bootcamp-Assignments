{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914545b0",
   "metadata": {},
   "source": [
    "1.\tWhat are Vanilla autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17528430",
   "metadata": {},
   "source": [
    "Vanilla autoencoders are neural network models that are trained to reconstruct their input data. They consist of an encoder network that maps the input data into a compressed representation (latent code), and a decoder network that maps the compressed representation back to the original input space. The objective of an autoencoder is to minimize the reconstruction error between the original input data and the reconstructed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e00e5",
   "metadata": {},
   "source": [
    "2.\tWhat are Sparse autoencoders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c41c5",
   "metadata": {},
   "source": [
    "Sparse autoencoders are a type of autoencoder that is designed to learn a compressed representation of data, while also encouraging sparsity in the learned representation. Sparsity refers to the property of having many values close to zero in the learned representation, effectively encoding only the most important features of the input data. This can be useful in situations where the input data has a large number of features, and the goal is to identify a smaller number of relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a20709",
   "metadata": {},
   "source": [
    "3.\tWhat are Denoising autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b6570",
   "metadata": {},
   "source": [
    "Denoising autoencoders are a type of autoencoder designed to learn robust representations of data by reconstructing the original input from a noisy or corrupted version of it. The idea is to force the model to learn the underlying structure of the data in order to reconstruct it accurately, even in the presence of noise or other forms of corruption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111a70d",
   "metadata": {},
   "source": [
    "4.\tWhat are Convolutional autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eed698",
   "metadata": {},
   "source": [
    "Convolutional autoencoders are a type of autoencoder that uses convolutional neural network (CNN) layers to encode and decode data. They are particularly well-suited for working with image data, but can also be used for other types of sequential data such as audio or video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d20fa",
   "metadata": {},
   "source": [
    "5.\tWhat are Stacked autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9356fe0a",
   "metadata": {},
   "source": [
    "Stacked autoencoders are a type of neural network that consists of multiple layers of autoencoder units stacked on top of each other. Each layer is an unsupervised learning algorithm that learns to encode the input data into a low-dimensional representation, and then decode it back to the original input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fbd57",
   "metadata": {},
   "source": [
    "6.\tExplain how to generate sentences using LSTM autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64666268",
   "metadata": {},
   "source": [
    "LSTM autoencoders can be used to generate new sentences by training the model on a corpus of text and then sampling from the learned distribution of sentence representations. \n",
    "\n",
    "- Preprocess the text corpus\n",
    "- Convert sentences to sequences of word embeddings\n",
    "- Train the LSTM autoencoder\n",
    "- Generate new sentences\n",
    "- Refine the generated sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407f2aa",
   "metadata": {},
   "source": [
    "7.\tExplain Extractive summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26b064",
   "metadata": {},
   "source": [
    "Extractive summarization is a technique used to create a summary of a larger text by selecting and extracting the most important and relevant sentences from the original text. This approach involves identifying the most informative sentences and presenting them as a condensed version of the original text. The goal of extractive summarization is to preserve the meaning and information of the original text while reducing its length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec823d",
   "metadata": {},
   "source": [
    "8.\tExplain Abstractive summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff98bfd",
   "metadata": {},
   "source": [
    "Abstractive summarization is a technique in natural language processing that involves summarizing a text document by generating a new summary that captures the most important information and meaning from the original text. Unlike extractive summarization, which selects and combines key phrases or sentences from the original document, abstractive summarization uses natural language generation techniques to create a summary that is not limited to the words and phrases present in the original document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819f6fc",
   "metadata": {},
   "source": [
    "9.\tExplain Beam search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c346814",
   "metadata": {},
   "source": [
    "Beam search is a search algorithm used to find the most likely sequence of tokens given a sequence of input tokens and a language model. It is commonly used in natural language processing tasks such as machine translation and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d709da",
   "metadata": {},
   "source": [
    "10.\tExplain Length normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7236480",
   "metadata": {},
   "source": [
    "Length normalization is a technique used in natural language processing (NLP) to address the problem of bias towards shorter or longer sentences in text generation tasks such as machine translation and text summarization. It is based on the observation that longer sentences tend to have lower probabilities due to their higher number of words, whereas shorter sentences tend to have higher probabilities due to their lower number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f46447",
   "metadata": {},
   "source": [
    "11.\tExplain Coverage normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da70282",
   "metadata": {},
   "source": [
    "Coverage normalization is a technique used in sequence-to-sequence models, particularly in neural machine translation. It is used to address the problem of repetition and/or omission of words or phrases in the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5585f59",
   "metadata": {},
   "source": [
    "12.\tExplain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cfdf58",
   "metadata": {},
   "source": [
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of evaluation metrics used to measure the quality of text summarization systems. ROUGE compares an automatically generated summary against a reference or human-generated summary to determine how well the system performed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
