{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c20a57",
   "metadata": {},
   "source": [
    "1.\tExplain the architecture of BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef57ba4",
   "metadata": {},
   "source": [
    "BERT, or Bidirectional Encoder Representations from Transformers, is a language model developed by Google that has achieved state-of-the-art results on a variety of natural language processing (NLP) tasks. BERT uses only the encoder part of the transformer model, and pretrains it on a large corpus of text using a masked language modeling task and a next sentence prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc82be",
   "metadata": {},
   "source": [
    "2.\tExplain Masked Language Modeling (MLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c74fdd",
   "metadata": {},
   "source": [
    "Masked Language Modeling (MLM) is a pretraining task used in the BERT (Bidirectional Encoder Representations from Transformers) model for unsupervised learning. In MLM, some of the tokens in a given input sequence are randomly masked, and the goal is to predict the original value of the masked tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d8340",
   "metadata": {},
   "source": [
    "3.\tExplain Next Sentence Prediction (NSP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d4b38",
   "metadata": {},
   "source": [
    "Next Sentence Prediction (NSP) is a pre-training task used in some language models, such as BERT (Bidirectional Encoder Representations from Transformers), to learn contextualized word embeddings. The goal of the NSP task is to predict whether one sentence is the next sentence in the original text given two input sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42a437",
   "metadata": {},
   "source": [
    "4.\tWhat is Matthews evaluation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083b03b",
   "metadata": {},
   "source": [
    "The Matthews correlation coefficient (MCC) is a metric that measures the quality of binary classifications, especially when the classes are imbalanced. It is commonly used in natural language processing (NLP) tasks such as sentiment analysis, named entity recognition, and text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ea59e",
   "metadata": {},
   "source": [
    "5.\tWhat is Matthews Correlation Coefficient (MCC)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410169e",
   "metadata": {},
   "source": [
    "Matthews correlation coefficient (MCC) is a metric used to evaluate the performance of binary classification models. It takes into account true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions. MCC ranges from -1 to 1, with 1 indicating perfect classification, 0 indicating no better than random classification, and -1 indicating complete disagreement between the predictions and the actual labels. MCC is particularly useful when the classes are imbalanced or when the accuracy is not an informative metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45151435",
   "metadata": {},
   "source": [
    "6.\tExplain Semantic Role Labeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6f6231",
   "metadata": {},
   "source": [
    "Semantic Role Labeling (SRL) is a natural language processing task that involves identifying the semantic roles of various phrases in a sentence and labeling them with their corresponding labels. The task aims to identify the relationship between words in a sentence and their roles with respect to the action or event being described."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592ca16",
   "metadata": {},
   "source": [
    "7.\tWhy Fine-tuning a BERT model takes less time than pretraining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020024a3",
   "metadata": {},
   "source": [
    "Fine-tuning a BERT model takes less time than pretraining because during pretraining, the BERT model is trained on a massive amount of unlabeled data to learn general language representations, while during fine-tuning, the pre-trained model is adapted to a specific downstream task by training on a smaller labeled dataset. Fine-tuning the pre-trained BERT model involves modifying only the last few layers of the model to predict the task-specific output, while the lower-level representations learned during pretraining are retained. Therefore, the fine-tuning process requires less computational resources and training time compared to the pretraining process, making it a practical approach for many NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af6355",
   "metadata": {},
   "source": [
    "8.\tRecognizing Textual Entailment (RTE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd695f",
   "metadata": {},
   "source": [
    "Recognizing Textual Entailment (RTE) is the task of determining whether one text (the \"hypothesis\") can be inferred from another text (the \"premise\"). It is a natural language processing task that has applications in many areas, such as information retrieval, question answering, and text summarization. The RTE task is typically formulated as a binary classification problem, where the goal is to predict whether the hypothesis can be entailed from the premise or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854cd27f",
   "metadata": {},
   "source": [
    "9.\tExplain the decoder stack of  GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7b51f",
   "metadata": {},
   "source": [
    "The decoder stack of GPT (Generative Pre-trained Transformer) models is a stack of identical decoder layers, where each layer has two sub-layers. The first sub-layer is a multi-head self-attention mechanism, and the second sub-layer is a position-wise feedforward network. In addition to these two sub-layers, the decoder stack also has a residual connection and layer normalization around each of the two sub-layers. The output of each decoder layer is fed to the next decoder layer, and the final output of the last decoder layer is used for the next step of the downstream task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
