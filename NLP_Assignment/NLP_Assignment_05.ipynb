{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f11f83",
   "metadata": {},
   "source": [
    "1.\tWhat are Sequence-to-sequence models?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65d12f",
   "metadata": {},
   "source": [
    "Sequence-to-sequence (seq2seq) models are a type of neural network architecture that can process variable-length input sequences and produce variable-length output sequences. They consist of two main components: an encoder that processes the input sequence and a decoder that generates the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5cf2ed",
   "metadata": {},
   "source": [
    "2.\tWhat are the Problem with Vanilla RNNs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769d86d",
   "metadata": {},
   "source": [
    "Vanilla RNNs suffer from a few problems \n",
    "\n",
    "* Vanishing gradients\n",
    "* Exploding gradients\n",
    "* Lack of memory\n",
    "* Difficulty with learning long-term dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e6a2f",
   "metadata": {},
   "source": [
    "3.\tWhat is Gradient clipping?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa15538",
   "metadata": {},
   "source": [
    "Gradient clipping is a technique used to prevent the gradients used for updating the weights of a neural network from becoming too large during training. It involves setting a threshold value for the gradient norm and scaling the gradients down to ensure that the norm does not exceed the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ab21d",
   "metadata": {},
   "source": [
    "4.\tExplain Attention mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f9f60",
   "metadata": {},
   "source": [
    "The attention mechanism is a technique used in deep learning to improve the performance of neural networks on tasks involving sequential data, such as machine translation, speech recognition, and image captioning. It allows the model to selectively focus on certain parts of the input sequence when making predictions, rather than relying on a fixed-length representation of the entire sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4cbae",
   "metadata": {},
   "source": [
    "5.\tExplain Conditional random fields (CRFs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b35a2",
   "metadata": {},
   "source": [
    "Conditional Random Fields (CRFs) is a probabilistic framework for supervised learning of sequence labeling tasks. It is an extension of the hidden Markov model (HMM) which models pairwise dependencies between adjacent labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d2de7",
   "metadata": {},
   "source": [
    "6.\tExplain self-attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089824b",
   "metadata": {},
   "source": [
    "Self-attention is a mechanism used in deep learning to weigh different parts of an input sequence differently based on the importance of each part in producing the output. It is often used in natural language processing tasks such as machine translation and text summarization, where the length of the input sequence can vary and different parts of the sequence may be more or less important for producing the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a776521",
   "metadata": {},
   "source": [
    "7.\tWhat is Bahdanau Attention?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2805fb40",
   "metadata": {},
   "source": [
    "Bahdanau Attention is a type of attention mechanism used in sequence-to-sequence models for natural language processing tasks such as machine translation. In Bahdanau Attention, the attention mechanism learns to weigh the importance of different parts of the input sequence when decoding a specific output. It does this by computing a set of attention scores, which are scalar values representing the relevance of each input element to the current decoding step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44485a8b",
   "metadata": {},
   "source": [
    "8.\tWhat is a Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac946b6",
   "metadata": {},
   "source": [
    "A Language Model (LM) is a statistical model that is trained on a corpus of text and can be used to predict the probability of a sequence of words. The goal of a language model is to learn the structure of natural language and capture the regularities and patterns in the data. The model can then be used for various tasks, such as speech recognition, machine translation, text generation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e0e47",
   "metadata": {},
   "source": [
    "9.\tWhat is Multi-Head Attention?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b962c3",
   "metadata": {},
   "source": [
    "Multi-Head Attention is an attention mechanism used in deep learning models, especially in Transformer-based architectures, that allows the model to attend to different positions in the input sequence which can be especially useful in tasks such as machine translation, where the model needs to attend to both the source and target language simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457536f3",
   "metadata": {},
   "source": [
    "10.\tWhat is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed1e73",
   "metadata": {},
   "source": [
    "Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the quality of machine-translated text by comparing it with one or more human reference translations. BLEU measures the similarity between the machine-generated output and the human reference translations by comparing their n-gram overlaps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
