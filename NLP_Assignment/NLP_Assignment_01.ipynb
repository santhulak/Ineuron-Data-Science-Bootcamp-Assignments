{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad28592",
   "metadata": {},
   "source": [
    "1.\tExplain One-Hot Encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde7db1",
   "metadata": {},
   "source": [
    "One-hot encoding is a common technique used in machine learning and data analysis to represent categorical variables as numerical data. In this technique, each unique category or value of a categorical variable is converted into a binary vector, where each element of the vector represents a possible category, and only one element is 1, indicating the presence of that category.\n",
    "\n",
    "One-hot encoding is useful when dealing with categorical variables, such as product categories, locations, or customer types, that do not have a natural numerical representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa4a65",
   "metadata": {},
   "source": [
    "2.\tExplain Bag of Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e61534",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) is a technique used in natural language processing (NLP) and text analysis to represent text documents as numerical vectors, which can be used as input to machine learning algorithms. In BoW, a document is first preprocessed to remove stopwords (common words that are unlikely to add any significant meaning to the text), punctuations, and other irrelevant characters. The remaining words are then tokenized, i.e., they are split into individual words or phrases. The resulting set of words is called the vocabulary.\n",
    "\n",
    "Next, a vector is created for each document, where each element of the vector represents a word in the vocabulary. The value of each element corresponds to the frequency of the corresponding word in the document. For example, consider the following two documents:\n",
    "\n",
    "Document 1: \"The cat in the hat\"\n",
    "\n",
    "Document 2: \"The dog chased the cat\"\n",
    "\n",
    "The vocabulary for these documents would be: \"the\", \"cat\", \"in\", \"hat\", \"dog\", \"chased\". The BoW representation of these documents would be:\n",
    "\n",
    "Document 1: [2, 1, 1, 1, 0, 0]\n",
    "\n",
    "Document 2: [2, 1, 0, 0, 1, 1]\n",
    "\n",
    "One limitation of BoW is that it does not capture the context and meaning of words in the text, which can be important in some NLP tasks. However, it is a simple and effective method for many text analysis tasks, especially when the focus is on word frequency rather than meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600990f3",
   "metadata": {},
   "source": [
    "3.\tExplain Bag of N-Grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c7855",
   "metadata": {},
   "source": [
    "The Bag of N-Grams is an extension of the Bag of Words (BoW) technique used in natural language processing (NLP) and text analysis to capture not only individual words but also sequences of n consecutive words, called n-grams. In BoW, each document is represented by a vector of word frequencies. In Bag of N-Grams, each document is represented by a vector of n-gram frequencies, where an n-gram is a contiguous sequence of n words.\n",
    "\n",
    "For example, consider the following sentence:\n",
    "\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "The unigrams (n=1) for this sentence would be: \"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\".\n",
    "\n",
    "The bigrams (n=2) would be: \"the quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", \"the lazy\", \"lazy dog\".\n",
    "\n",
    "The trigrams (n=3) would be: \"the quick brown\", \"quick brown fox\", \"brown fox jumps\", \"fox jumps over\", \"jumps over the\", \"over the lazy\", \"the lazy dog\".\n",
    "\n",
    "The frequencies of each n-gram can then be counted to create a vector representing the document.\n",
    "\n",
    "The Bag of N-Grams representation can capture more contextual information than the Bag of Words, especially for languages where the meaning of words can depend on their neighboring words. However, it also requires more memory and computational resources, especially for large values of n.\n",
    "\n",
    "The choice of the value of n depends on the specific NLP task and the size of the corpus. Unigrams are commonly used for tasks such as sentiment analysis and topic modeling, while larger values of n may be useful for tasks such as named entity recognition or machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa105d",
   "metadata": {},
   "source": [
    "4.\tExplain TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e098255",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency-Inverse Document Frequency, and it is a commonly used technique in natural language processing (NLP) and text mining for calculating the importance of a word or phrase in a document or corpus of documents. The basic idea behind TF-IDF is to weigh a term's frequency in a document against its frequency in the entire corpus.\n",
    "\n",
    "TF (Term Frequency) measures the frequency of a term in a document. It is calculated as the number of times a term appears in a document divided by the total number of terms in the document. This value reflects the importance of a term within a single document.\n",
    "\n",
    "IDF (Inverse Document Frequency) measures the rarity of a term across the entire corpus. It is calculated as the logarithm of the total number of documents in the corpus divided by the number of documents containing the term. This value reflects the importance of a term in distinguishing a particular document from the rest of the corpus.\n",
    "\n",
    "The product of the two values, TF and IDF, gives the TF-IDF weight of a term in a document. TF-IDF can also be used to preprocess the text before applying other techniques such as clustering, classification, or topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8eb36",
   "metadata": {},
   "source": [
    "5.\tWhat is OOV problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e498c62",
   "metadata": {},
   "source": [
    "OOV stands for Out-of-Vocabulary, and it refers to a problem that arises in natural language processing (NLP) when a word or phrase that is not present in the vocabulary of a language model is encountered in a text. This can happen, for example, when dealing with new or uncommon words, misspellings, or words in a language or dialect not covered by the model.\n",
    "\n",
    "The OOV problem can cause errors in various NLP tasks, such as machine translation, text classification, and speech recognition, among others. For example, if an OOV word is encountered in a sentence during machine translation, the model may not be able to accurately translate that word, leading to errors in the translated output. Similarly, in speech recognition, an OOV word may be incorrectly recognized as a different word, leading to errors in the transcribed text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ee698",
   "metadata": {},
   "source": [
    "6.\tWhat are word embeddings?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9951e69",
   "metadata": {},
   "source": [
    "Word embeddings are a popular technique used in natural language processing (NLP) to represent words as dense vectors of real numbers in a high-dimensional space. The basic idea behind word embeddings is to capture the meaning and semantic relationships between words in a way that can be used as input to machine learning algorithms for various NLP tasks such as language modeling, sentiment analysis, and machine translation, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3f543",
   "metadata": {},
   "source": [
    "7.\tExplain Continuous bag of words (CBOW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f732c",
   "metadata": {},
   "source": [
    "Continuous Bag of Words (CBOW) is a type of word embedding model used in natural language processing (NLP) to represent words as dense vectors in a high-dimensional space. CBOW is a neural network architecture that takes a context of words and predicts the target word in the center of the context.\n",
    "\n",
    "In CBOW, the input layer of the neural network takes a one-hot encoding of the context words, which are passed through a hidden layer to produce an embedding of the context. The embedding is then fed through another hidden layer to predict the target word. The objective of the model is to minimize the difference between the predicted and actual target word, using techniques such as softmax or negative sampling.\n",
    "\n",
    "CBOW is faster to train and generally performs better on tasks that require a higher level of syntactic and semantic information, such as part-of-speech tagging and named entity recognition. However, it may not perform as well on tasks that require a higher level of context sensitivity, such as sentiment analysis.\n",
    "\n",
    "CBOW has become a popular technique in NLP and is used in many state-of-the-art systems for tasks such as language modeling, machine translation, and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad690f13",
   "metadata": {},
   "source": [
    "8.\tExplain SkipGram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b086e0",
   "metadata": {},
   "source": [
    "Skip-gram is a type of word embedding model used in natural language processing (NLP) to represent words as dense vectors in a high-dimensional space. \n",
    "\n",
    "In Skip-gram, the input layer of the neural network takes a one-hot encoding of the target word, which is then passed through a hidden layer to produce an embedding of the target. The embedding is then fed through another hidden layer to predict the context words. The objective of the model is to minimize the difference between the predicted and actual context words, using techniques such as softmax or negative sampling.\n",
    "\n",
    "Skip-gram is trained on a large corpus of text, and the resulting word embeddings capture the meaning and relationships between words based on their co-occurrence in the text. The size of the context window can be adjusted to capture different levels of semantic and syntactic information.\n",
    "\n",
    "Skip-gram has become a popular technique in NLP and is used in many state-of-the-art systems for tasks such as language modeling, machine translation, and speech recognition. It is one of the most widely used word embedding models, alongside CBOW and GloVe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b697d",
   "metadata": {},
   "source": [
    "9.\tExplain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dcdc71",
   "metadata": {},
   "source": [
    "GloVe (Global Vectors) is a type of word embedding model used in natural language processing (NLP) to represent words as dense vectors in a high-dimensional space. GloVe is designed to capture both global and local information about word co-occurrence statistics.\n",
    "\n",
    "GloVe embeddings have been used in many state-of-the-art NLP systems for tasks such as sentiment analysis, named entity recognition, and machine translation. They have also been shown to improve the performance of various NLP models, including recurrent neural networks and convolutional neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
