{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6300fd7",
   "metadata": {},
   "source": [
    "1.\tWhat are Corpora?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd2305",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), a corpus (plural \"corpora\") refers to a collection of texts or other linguistic data, typically stored in electronic format. Corpora are used as a primary resource for studying language, as they provide a large and diverse set of language data that can be analyzed and processed.\n",
    "\n",
    "Corpora can come in many different forms, ranging from small, specialized collections of texts to large, general-purpose collections of texts. Some corpora are compiled from specific sources, such as newspapers, academic journals, or social media platforms, while others are more general and represent a broad range of language use.\n",
    "\n",
    "Corpora can be annotated or unannotated. Annotated corpora include additional information about the text, such as part-of-speech tags, syntactic structures, or named entities. Unannotated corpora do not contain any additional information about the text beyond the raw text itself.\n",
    "\n",
    "Corpora are used for a variety of NLP tasks, including language modeling, machine translation, sentiment analysis, information extraction, and many others. They are also used to train and evaluate machine learning models for these tasks, as well as for statistical analysis of language use and variation.\n",
    "\n",
    "Overall, corpora are an essential resource for researchers and practitioners in NLP, providing a foundation for understanding and modeling language use in various contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18563bd",
   "metadata": {},
   "source": [
    "2.\tWhat are Tokens?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb95fb",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), a token refers to an individual unit of meaning that is extracted from a larger body of text. Tokens are typically words or punctuation marks that are separated by whitespace or other delimiters, such as commas or periods.\n",
    "\n",
    "For example, consider the sentence \"The quick brown fox jumped over the lazy dog.\" In this sentence, there are nine tokens: \"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", and \"dog\". Each of these tokens represents a discrete unit of meaning within the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24a414",
   "metadata": {},
   "source": [
    "3.\tWhat are Unigrams, Bigrams, Trigrams?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b30a1a",
   "metadata": {},
   "source": [
    "Unigrams, bigrams, and trigrams are different types of n-grams, which are contiguous sequences of n items (usually words) from a body of text. These n-grams are often used in natural language processing (NLP) to represent the structure and patterns of language use in a text.\n",
    "\n",
    "Unigrams: A unigram is a single word, or a sequence of characters delimited by white space. For example, in the sentence \"The quick brown fox jumps over the lazy dog\", the unigrams are \"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", and \"dog\".\n",
    "\n",
    "Bigrams: A bigram is a sequence of two adjacent words in a text. For example, in the same sentence, some of the bigrams are \"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", \"the lazy\", and \"lazy dog\".\n",
    "\n",
    "Trigrams: A trigram is a sequence of three adjacent words in a text. For example, in the same sentence, some of the trigrams are \"The quick brown\", \"quick brown fox\", \"brown fox jumps\", \"fox jumps over\", \"jumps over the\", \"over the lazy\", and \"the lazy dog\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b6f1ab",
   "metadata": {},
   "source": [
    "4.\tHow to generate n-grams from text?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab816918",
   "metadata": {},
   "source": [
    "Tokenize the text: Split the text into individual words or tokens, using a tokenizer or a regular expression. You can also preprocess the text by converting all the characters to lowercase and removing punctuation, stop words, and other irrelevant elements.\n",
    "\n",
    "Create n-grams: Create n-grams by sliding a window of size n over the tokenized text. At each position, extract a sequence of n adjacent tokens, and add it to a list of n-grams.\n",
    "\n",
    "Count the frequency of n-grams: Count the frequency of each n-gram in the text, and store it in a dictionary or a frequency table. You can also normalize the counts by dividing by the total number of n-grams in the text, or by the total number of occurrences of each individual token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f59216",
   "metadata": {},
   "source": [
    "5.\tExplain Lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a6dc9",
   "metadata": {},
   "source": [
    "Lemmatization is a process of reducing a word to its base or dictionary form, called a lemma. In other words, it is the process of converting words into their canonical form, which helps to group together different forms of a word and improve the accuracy of natural language processing (NLP) tasks such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "For example, the lemma of the word \"running\" is \"run\", and the lemma of the word \"rocks\" is \"rock\". Lemmatization involves removing the inflectional endings of a word, such as -s, -ed, -ing, and so on, to get its base form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a50e8a",
   "metadata": {},
   "source": [
    "6.\tExplain Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591dc0de",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its base or root form, called a stem, by removing the suffixes of the word. The stem may not be a valid word on its own, but it represents the core meaning of the word.\n",
    "\n",
    "For example, the stem of the word \"running\" is \"run\", and the stem of the word \"jumped\" is \"jump\". Stemming can help to reduce the dimensionality of text data by grouping together different forms of a word, which can be useful for tasks such as information retrieval and document classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2983f09",
   "metadata": {},
   "source": [
    "7.\tExplain Part-of-speech (POS) tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c029e6",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the process of assigning grammatical labels, or tags, to words in a text based on their syntactic category and function in the sentence. These tags identify the part of speech of each word, such as noun, verb, adjective, or adverb, as well as other information such as tense, number, and gender.\n",
    "\n",
    "POS tagging is a fundamental task in natural language processing (NLP) and is used in various applications such as information retrieval, text-to-speech conversion, and machine translation. POS tagging can be performed using different approaches, such as rule-based, stochastic, or deep learning-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167399c",
   "metadata": {},
   "source": [
    "8.\tExplain Chunking or shallow parsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a035a",
   "metadata": {},
   "source": [
    "Chunking, also known as shallow parsing, is a technique in natural language processing (NLP) that involves identifying and grouping together contiguous words in a text that have a specific grammatical structure or meaning. This process is used to extract meaningful chunks of information from a sentence or document, such as noun phrases, verb phrases, or prepositional phrases.\n",
    "\n",
    "The basic idea behind chunking is to use patterns of POS tags to identify sequences of words that have a certain syntactic structure. For example, a noun phrase might consist of a determiner followed by one or more adjectives and a noun, while a verb phrase might consist of a verb followed by one or more adverbs and/or direct objects. These patterns can be defined using regular expressions or other pattern matching techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663bde58",
   "metadata": {},
   "source": [
    "9.\tExplain Noun Phrase (NP) chunking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d0a1e",
   "metadata": {},
   "source": [
    "Noun Phrase (NP) chunking is a type of chunking or shallow parsing that involves identifying and grouping together contiguous words in a text that form a noun phrase. A noun phrase typically consists of a noun and any words that modify or describe it, such as adjectives, determiners, and prepositional phrases.\n",
    "\n",
    "NP chunking can be useful for various NLP tasks, such as named entity recognition, information extraction, and text classification. For example, in named entity recognition, identifying and extracting noun phrases can help to identify proper nouns and named entities, which are often composed of multiple words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a2be2",
   "metadata": {},
   "source": [
    "10.\tExplain Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a98ab3",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a natural language processing (NLP) technique that involves identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, and dates.\n",
    "\n",
    "The task of NER involves two main steps: first, identifying the boundaries of the named entity within the text, and second, assigning a category label to the entity. For example, given the sentence \"Barack Obama was born in Hawaii on August 4, 1961\", an NER system would identify \"Barack Obama\" as a person name, \"Hawaii\" as a location, and \"August 4, 1961\" as a date."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
